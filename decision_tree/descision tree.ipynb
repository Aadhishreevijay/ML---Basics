{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a20b701c-5c83-4119-aae0-050754e9915b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DecisionTreeClassifier: 74.68%\n",
      "Prediction: Diabetic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data from CSV file\n",
    "df = pd.read_csv(\"diabetes.csv\")\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the DecisionTreeClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of DecisionTreeClassifier: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Example usage for prediction\n",
    "new_data = np.array([[6, 148, 72, 35, 0, 33.6, 0.627, 50]])\n",
    "prediction = clf.predict(new_data)\n",
    "print(f\"Prediction: {'Diabetic' if prediction[0] == 1 else 'Non-Diabetic'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2011011e-748d-4542-8519-b35fc6950d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of custom Decision Tree: 58.44%\n",
      "Prediction: Diabetic\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Entropy calculation\n",
    "def entropy(y):\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    probabilities = counts / len(y)\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy\n",
    "\n",
    "# Information gain calculation\n",
    "def information_gain(X, y, feature):\n",
    "    entropy_parent = entropy(y)\n",
    "    unique_values = np.unique(X[:, feature])\n",
    "    weighted_entropy_children = 0\n",
    "    for value in unique_values:\n",
    "        child_indices = np.where(X[:, feature] == value)[0]\n",
    "        child_entropy = entropy(y[child_indices])\n",
    "        weighted_entropy_children += (len(child_indices) / len(y)) * child_entropy\n",
    "    information_gain = entropy_parent - weighted_entropy_children\n",
    "    return information_gain\n",
    "\n",
    "# Find the best feature to split on\n",
    "def find_best_split(X, y):\n",
    "    best_information_gain = -1\n",
    "    best_feature = None\n",
    "    for feature in range(X.shape[1]):\n",
    "        current_information_gain = information_gain(X, y, feature)\n",
    "        if current_information_gain > best_information_gain:\n",
    "            best_information_gain = current_information_gain\n",
    "            best_feature = feature\n",
    "    return best_feature\n",
    "\n",
    "# Build the decision tree\n",
    "def build_tree(X, y):\n",
    "    if len(np.unique(y)) == 1:\n",
    "        return {'leaf': True, 'class': y[0]}\n",
    "    if X.shape[1] == 0:\n",
    "        return {'leaf': True, 'class': np.bincount(y).argmax()}\n",
    "    best_feature = find_best_split(X, y)\n",
    "    unique_values = np.unique(X[:, best_feature])\n",
    "    tree = {'feature': best_feature, 'children': {}}\n",
    "    for value in unique_values:\n",
    "        child_indices = np.where(X[:, best_feature] == value)[0]\n",
    "        child_X = X[child_indices]\n",
    "        child_y = y[child_indices]\n",
    "        tree['children'][value] = build_tree(child_X, child_y)\n",
    "    return tree\n",
    "\n",
    "# Predict a single sample\n",
    "# Predict a single sample\n",
    "def predict_single(tree, sample):\n",
    "    if 'leaf' in tree and tree['leaf']:\n",
    "        return tree['class']\n",
    "    else:\n",
    "        feature = tree['feature']\n",
    "        value = sample[feature]\n",
    "        if value in tree['children']:\n",
    "            return predict_single(tree['children'][value], sample)\n",
    "        else:\n",
    "            # If value not found in training data, return majority class of current node\n",
    "            child_classes = [node['class'] for node in tree['children'].values() if 'class' in node]\n",
    "            return max(child_classes, key=child_classes.count)\n",
    "\n",
    "\n",
    "# Predict multiple samples\n",
    "def predict(tree, X):\n",
    "    predictions = []\n",
    "    for sample in X:\n",
    "        predictions.append(predict_single(tree, sample))\n",
    "    return predictions\n",
    "\n",
    "# Calculate accuracy\n",
    "def calculate_accuracy(tree, X_test, y_test):\n",
    "    predictions = predict(tree, X_test)\n",
    "    correct_predictions = np.sum(predictions == y_test)\n",
    "    accuracy = correct_predictions / len(y_test)\n",
    "    return accuracy\n",
    "\n",
    "# Load data from CSV file\n",
    "df = pd.read_csv(\"diabetes.csv\")\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build decision tree\n",
    "tree = build_tree(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = calculate_accuracy(tree, X_test, y_test)\n",
    "print(f\"Accuracy of custom Decision Tree: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Example usage\n",
    "new_data = np.array([[6, 148, 72, 35, 0, 33.6, 0.627, 50]])\n",
    "prediction = predict(tree, new_data)\n",
    "print(f\"Prediction: {'Diabetic' if prediction[0] == 1 else 'Non-Diabetic'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6c9eb93e-cf76-4ce7-87ad-623fbb3c3182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d8d70e18-5161-4411-a714-2a90982d16a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    unique,counts=np.unique(y,return_counts=True)\n",
    "    prob=counts/len(y)\n",
    "    entropy=-np.sum(prob*np.log2(prob))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b0a4f0a7-2118-4c48-8183-f73e47901cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info(x,y,feature):\n",
    "    ento_p=entropy(y)\n",
    "    unique_vals=np.unique(x[:,feature])\n",
    "    wt=0\n",
    "    for value in unique_vals:\n",
    "        child_indices=np.where(x[:,feature]==value)[0]\n",
    "        ento_c=entropy(y[child_indices])\n",
    "        wt+=(len(child_indices)/len(y))*ento_c\n",
    "    i_g=ento_p-wt\n",
    "    return i_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5c665fa4-20bf-460e-a568-7d109c0e25bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_split(x,y):\n",
    "    best_ig=-1\n",
    "    best_feature =None\n",
    "    for feature in range(x.shape[1]):\n",
    "        current_ig=info(x,y,feature)\n",
    "        if current_ig>best_ig:\n",
    "            best_ig=current_ig\n",
    "            best_feature=feature\n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4080205d-b274-42dc-bec3-659d41f152ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def btree(x,y):\n",
    "    if len(np.unique(y))==1:\n",
    "        return {'leaf':True ,'class':y[0]}\n",
    "    if x.shape[1]==0:\n",
    "        return {'leaf' : True,'class':np.bincount(y).argmax()}\n",
    "    b_feature=best_split(x,y)\n",
    "    unique_values=np.unique(x[:,b_feature])\n",
    "    tree={'feature':b_feature,'children':{}}\n",
    "    for value in unique_values:\n",
    "        indice=np.where(x[:,b_feature]==value)[0]\n",
    "        x_child=x[indice]\n",
    "        y_child=y[indice]\n",
    "        tree['children'][value]=btree(x_child,y_child)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0da9fbe2-a54a-4851-b751-3c3c4973bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre(tree, sample):\n",
    "    if 'leaf' in tree and tree['leaf']:\n",
    "        return tree['class']\n",
    "    else:\n",
    "        feature = tree['feature']\n",
    "        value = sample[feature]\n",
    "        if value in tree['children']:\n",
    "            return pre(tree['children'][value], sample)\n",
    "        else:\n",
    "            # If value not found in training data, return majority class of current node\n",
    "            child_classes = [node['class'] for node in tree['children'].values() if 'class' in node]\n",
    "            return max(child_classes, key=child_classes.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e79712ff-c16e-4ea6-a97e-ac107e3ab80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tree,x):\n",
    "    predictions=[]\n",
    "    for sample in x:\n",
    "        predictions.append(pre(tree,sample))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "635e9a06-6383-47dc-a719-6b31c4b0a5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(tree,x_test,y_test):\n",
    "    predictions=predict(tree,x_test)\n",
    "    corr=np.sum(predictions==y_test)\n",
    "    accuracy=corr/len(y_test)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "43c8e3d6-a650-41f9-bbb4-84b6d12141cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('diabetes.csv')\n",
    "x=data.iloc[:,:-1].values\n",
    "y=data.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6db561bb-0352-42e9-acca-9f6b71392d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5844155844155844\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to NoneType.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[0;32m      8\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy(tree, x_test, y_test)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy of custom Decision Tree: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     12\u001b[0m a\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m148\u001b[39m, \u001b[38;5;241m72\u001b[39m, \u001b[38;5;241m35\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m33.6\u001b[39m, \u001b[38;5;241m0.627\u001b[39m, \u001b[38;5;241m50\u001b[39m]])\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported format string passed to NoneType.__format__"
     ]
    }
   ],
   "source": [
    "# Split dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build decision tree\n",
    "tree = btree(x_train, y_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy(tree, x_test, y_test)\n",
    "print(f\"Accuracy of custom Decision Tree: {accuracy:.2f}%\")\n",
    "\n",
    "# Example usage\n",
    "a= np.array([[6, 148, 72, 35, 0, 33.6, 0.627, 50]])\n",
    "prediction = predict(tree, a)\n",
    "print(f\"Prediction: {'Diabetic' if prediction[0] == 1 else 'Non-Diabetic'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
